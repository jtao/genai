{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtao/genai/blob/main/2.%20LLM-Compound-Systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mko93UVbJJa"
      },
      "source": [
        "# LLM Orchestration: RAG, DSPy, and Vector Databases\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This Colab notebook provides a hands-on exploration of advanced LLM orchestration techniques, focusing on Retrieval-Augmented Generation (RAG), DSPy, vector databases, and chunking strategies. We'll be using Llama 3.1-8B as our base language model throughout this notebook.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsSNYeD5bLPw"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers bitsandbytes datasets torch langchain langchain_community langchain-huggingface faiss-cpu sentence-transformers dspy huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "vx6EDTJMFVQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKWA-bezbNZ6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import bitsandbytes\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer ,pipeline\n",
        "from datasets import load_dataset\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import dspy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMAyTuKwbUUh"
      },
      "outputs": [],
      "source": [
        "#Now, let's import the required libraries:\n",
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
        "# Create a text-generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "\n",
        "# Create a LangChain wrapper for the Hugging Face pipeline\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO_eQJ4NbVK8"
      },
      "source": [
        "## 1. Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "### 1.1 Setting up a Document Store\n",
        "#### Preparing Data for Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "In the world of large language models and AI-powered question-answering systems, Retrieval-Augmented Generation (RAG) has emerged as a powerful technique. RAG combines the strengths of retrieval-based and generation-based approaches, allowing models to access and utilize external knowledge when generating responses. This approach can significantly improve the accuracy and relevance of AI-generated content.\n",
        "\n",
        "The following code snippet demonstrates the essential steps in preparing data for a RAG system. We'll walk through the process of:\n",
        "\n",
        "1. Loading a dataset from a reliable source\n",
        "2. Converting the dataset into a suitable format for processing\n",
        "3. Splitting the text into manageable chunks\n",
        "4. Creating embeddings and storing them in a vector database\n",
        "\n",
        "This preparation pipeline is crucial for building an effective RAG system. It allows us to take raw text data and transform it into a format that can be quickly and efficiently searched when our model needs to retrieve relevant information.\n",
        "\n",
        "The code uses the Simple English Wikipedia dataset, which is an excellent resource for this purpose due to its broad coverage of topics and its use of straightforward language. We'll use tools from the Hugging Face ecosystem and the FAISS library to process and store our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAKChO1gbZfQ"
      },
      "outputs": [],
      "source": [
        "# Load a dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:1000]\")\n",
        "\n",
        "# Convert dataset to documents\n",
        "documents = [doc['text'] for doc in dataset]\n",
        "\n",
        "# Split the text into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=20)\n",
        "texts = text_splitter.create_documents(documents)\n",
        "\n",
        "# Create embeddings and store them in a vector database\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "db = FAISS.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_pgfy-ubbVq"
      },
      "source": [
        "\n",
        "### 1.2 Implementing RAG\n",
        "\n",
        "Now, let's implement a simple RAG system using LangChain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg-372c9bdTk"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Create a retrieval-based QA chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 3})\n",
        ")\n",
        "\n",
        "# Test the RAG system\n",
        "query = \"What is the capital of France?\"\n",
        "result = qa.run(query)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3kGvmAobfYX"
      },
      "source": [
        "### Exercise 1.3: Extend the RAG System\n",
        "\n",
        "Implement a multi-turn conversation system using RAG. This system should maintain context across multiple queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crUT8OrKbibz"
      },
      "outputs": [],
      "source": [
        "class ConversationalRAG:\n",
        "    def __init__(self, qa_chain):\n",
        "        self.qa_chain = qa_chain\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def ask(self, question):\n",
        "        context = \" \".join(self.conversation_history[-3:])  # Use last 3 exchanges as context\n",
        "        full_query = f\"Context: {context}\\n\\nQuestion: {question}\"\n",
        "        response = self.qa_chain.run(full_query)\n",
        "        self.conversation_history.append(f\"Q: {question}\\nA: {response}\")\n",
        "        return response\n",
        "\n",
        "# Create an instance of ConversationalRAG\n",
        "conv_rag = ConversationalRAG(qa)\n",
        "\n",
        "# Test the conversational RAG system\n",
        "print(conv_rag.ask(\"What is the capital of France?\"))\n",
        "print(conv_rag.ask(\"What is its population?\"))\n",
        "print(conv_rag.ask(\"Tell me about its famous landmarks.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQDTwCuwbj6V"
      },
      "source": [
        "\n",
        "## 2. Exploring DSPy\n",
        "\n",
        "### 2.1 Setting up DSPy\n",
        "\n",
        "Let's set up a basic DSPy environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKl5xuvfbmeV"
      },
      "outputs": [],
      "source": [
        "import dspy\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "class CustomLLM:\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
        "        self.pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.15\n",
        "        )\n",
        "\n",
        "    def basic_request(self, prompt, **kwargs):\n",
        "        response = self.pipeline(prompt, **kwargs)[0]['generated_text']\n",
        "        return response[len(prompt):]\n",
        "\n",
        "    def __call__(self, prompt, **kwargs):\n",
        "        return self.basic_request(prompt, **kwargs)\n",
        "\n",
        "# Create an instance of CustomLLM\n",
        "custom_llm = CustomLLM(model_name)  # or your specific model name\n",
        "\n",
        "# Configure DSPy to use our custom LLM\n",
        "dspy.settings.configure(lm=custom_llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSVx35xUbowh"
      },
      "source": [
        "### 2.2 Building a Basic DSPy Pipeline\n",
        "\n",
        "Let's create a simple question-answering pipeline using DSPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrlIEiZRbrDw"
      },
      "outputs": [],
      "source": [
        "import dspy\n",
        "\n",
        "class SimpleQA:\n",
        "    def __init__(self, lm):\n",
        "        self.lm = lm\n",
        "\n",
        "    def generate_answer(self, question):\n",
        "        prompt = f\"Question: {question}\\nAnswer:\"\n",
        "        return self.lm(prompt)\n",
        "\n",
        "    def __call__(self, question):\n",
        "        return self.generate_answer(question)\n",
        "\n",
        "# Assuming you've already set up your language model as 'custom_llm'\n",
        "# If not, you can create it like this:\n",
        "# custom_llm = CustomLLM(\"your-model-name\")\n",
        "\n",
        "# Create an instance of SimpleQA\n",
        "simple_qa = SimpleQA(dspy.settings.lm)\n",
        "\n",
        "# Test the QA system\n",
        "question = \"What is machine learning?\"\n",
        "result = simple_qa(question)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bw5PXzzbto5"
      },
      "source": [
        "### Exercise 2.3: Extend the DSPy Pipeline\n",
        "\n",
        "Implement a fact-checking module in the DSPy pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BpWBoWXbwiU"
      },
      "outputs": [],
      "source": [
        "import dspy\n",
        "import re\n",
        "\n",
        "class RobustFactCheckingQA:\n",
        "    def __init__(self, lm):\n",
        "        self.lm = lm\n",
        "\n",
        "    def generate_answer(self, question):\n",
        "        prompt = f\"Question: {question}\\nAnswer:\"\n",
        "        return self.lm(prompt)\n",
        "\n",
        "    def fact_check(self, question, answer):\n",
        "        prompt = f\"\"\"\n",
        "        Question: {question}\n",
        "        Given Answer: {answer}\n",
        "\n",
        "        Please fact-check the given answer and provide:\n",
        "        1. Factual Accuracy (as a percentage)\n",
        "        2. Explanation of your fact-check\n",
        "\n",
        "        Your response should follow this format:\n",
        "        Factual Accuracy: [percentage]\n",
        "        Explanation: [your explanation]\n",
        "        \"\"\"\n",
        "        return self.lm(prompt)\n",
        "\n",
        "    def parse_fact_check_result(self, result):\n",
        "        # Try to find factual accuracy using regex\n",
        "        accuracy_match = re.search(r'Factual Accuracy:?\\s*(\\d+%?)', result, re.IGNORECASE)\n",
        "        factual_accuracy = accuracy_match.group(1) if accuracy_match else 'N/A'\n",
        "\n",
        "        # Everything after \"Factual Accuracy\" line is considered explanation\n",
        "        explanation_parts = result.split('\\n')[1:]  # Skip the first line which should be Factual Accuracy\n",
        "        explanation = '\\n'.join(explanation_parts).strip()\n",
        "\n",
        "        return factual_accuracy, explanation\n",
        "\n",
        "    def __call__(self, question):\n",
        "        initial_answer = self.generate_answer(question)\n",
        "        fact_check_result = self.fact_check(question, initial_answer)\n",
        "\n",
        "        factual_accuracy, explanation = self.parse_fact_check_result(fact_check_result)\n",
        "\n",
        "        return {\n",
        "            'answer': initial_answer,\n",
        "            'factual_accuracy': factual_accuracy,\n",
        "            'explanation': explanation\n",
        "        }\n",
        "\n",
        "# Create an instance of RobustFactCheckingQA\n",
        "fact_checking_qa = RobustFactCheckingQA(dspy.settings.lm)\n",
        "\n",
        "# Test the fact-checking QA system\n",
        "question = \"Who invented the telephone?\"\n",
        "result = fact_checking_qa(question)\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(f\"Factual Accuracy: {result['factual_accuracy']}\")\n",
        "print(f\"Explanation: {result['explanation']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ljcu-IubzNT"
      },
      "source": [
        "## 3. Vector Databases and Efficient Retrieval\n",
        "\n",
        "### 3.1 Creating a Vector Database\n",
        "\n",
        "We'll use the SentenceTransformer model to create embeddings and FAISS to store them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT-Z06J7b2UW"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create some sample sentences\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a popular programming language for data science.\",\n",
        "    \"Natural language processing deals with the interaction between computers and human language.\",\n",
        "    \"Deep learning models are based on artificial neural networks with multiple layers.\"\n",
        "]\n",
        "\n",
        "# Create embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSAOlvfNb37s"
      },
      "source": [
        "\n",
        "### 3.2 Performing Similarity Search\n",
        "\n",
        "Now, let's perform a similarity search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVdb_2Yab6bN"
      },
      "outputs": [],
      "source": [
        "def similarity_search(query, index, model, sentences, k=2):\n",
        "    # Create a query embedding\n",
        "    query_embedding = model.encode([query])\n",
        "\n",
        "    # Perform the search\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"Most similar sentences:\")\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        print(f\"{i+1}. {sentences[idx]} (Distance: {distances[0][i]:.4f})\")\n",
        "\n",
        "# Test the similarity search\n",
        "similarity_search(\"AI and its applications\", index, model, sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8oarn0Fb8Q8"
      },
      "source": [
        "### 3.3 Chunking Strategies\n",
        "\n",
        "Let's implement a more advanced chunking strategy that considers sentence boundaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB3N2PYGb-yJ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def sentence_aware_chunks(text, max_chunk_size=200, overlap=20):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
        "            current_chunk += \" \" + sentence\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence[-overlap:] + \" \" + sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Test the chunking strategy\n",
        "long_text = \" \".join(sentences)\n",
        "chunks = sentence_aware_chunks(long_text)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SVP-i1UcA-y"
      },
      "source": [
        "### Exercise 3.4: Compare Chunking Strategies\n",
        "\n",
        "Implement a fixed-length chunking strategy and compare its performance with the sentence-aware approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGi-Woq3cDfM"
      },
      "outputs": [],
      "source": [
        "def fixed_length_chunks(text, chunk_size=200):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# Compare the two chunking strategies\n",
        "print(\"Sentence-aware chunks:\")\n",
        "sentence_chunks = sentence_aware_chunks(long_text)\n",
        "for i, chunk in enumerate(sentence_chunks):\n",
        "    print(f\"Chunk {i+1} (length {len(chunk)}): {chunk[:50]}...\")\n",
        "\n",
        "print(\"\\nFixed-length chunks:\")\n",
        "fixed_chunks = fixed_length_chunks(long_text)\n",
        "for i, chunk in enumerate(fixed_chunks):\n",
        "    print(f\"Chunk {i+1} (length {len(chunk)}): {chunk[:50]}...\")\n",
        "\n",
        "# Evaluate coherence (you may need to implement a more sophisticated coherence metric)\n",
        "def simple_coherence_score(chunks):\n",
        "    return sum(1 for chunk in chunks if chunk[-1] in '.!?') / len(chunks)\n",
        "\n",
        "print(f\"\\nSentence-aware coherence: {simple_coherence_score(sentence_chunks):.2f}\")\n",
        "print(f\"Fixed-length coherence: {simple_coherence_score(fixed_chunks):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-5q77YWcF0B"
      },
      "source": [
        "## 4. Integrating RAG, DSPy, and Vector Databases\n",
        "\n",
        "Now, let's bring everything together by creating an advanced RAG system that uses DSPy and our custom vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igx0maVvcfFV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class AdvancedRAG:\n",
        "    def __init__(self, faiss_index, sentences, model, lm):\n",
        "        \"\"\"\n",
        "        Initialize the AdvancedRAG system.\n",
        "\n",
        "        :param faiss_index: A pre-built FAISS index for efficient similarity search\n",
        "        :param sentences: A list of sentences or documents that correspond to the FAISS index\n",
        "        :param model: A sentence transformer model for encoding queries\n",
        "        :param lm: A language model for generating answers (e.g., GPT-2 or Llama)\n",
        "        \"\"\"\n",
        "        self.faiss_index = faiss_index\n",
        "        self.sentences = sentences\n",
        "        self.model = model  # Sentence transformer model\n",
        "        self.lm = lm  # Language model for answer generation\n",
        "\n",
        "    def retrieve(self, query, k=2):\n",
        "        \"\"\"\n",
        "        Retrieve the most relevant contexts for a given query.\n",
        "\n",
        "        :param query: The input question or query\n",
        "        :param k: The number of contexts to retrieve (default is 2)\n",
        "        :return: A list of the k most relevant sentences/documents\n",
        "        \"\"\"\n",
        "        # Encode the query using the sentence transformer model\n",
        "        query_embedding = self.model.encode([query])\n",
        "\n",
        "        # Perform a similarity search in the FAISS index\n",
        "        # This returns the distances and indices of the k nearest neighbors\n",
        "        distances, indices = self.faiss_index.search(query_embedding, k)\n",
        "\n",
        "        # Return the actual sentences/documents corresponding to the found indices\n",
        "        return [self.sentences[idx] for idx in indices[0]]\n",
        "\n",
        "    def generate_answer(self, question, context):\n",
        "        \"\"\"\n",
        "        Generate an answer to the question based on the retrieved context.\n",
        "\n",
        "        :param question: The input question\n",
        "        :param context: The retrieved relevant contexts\n",
        "        :return: The generated answer\n",
        "        \"\"\"\n",
        "        # Construct a prompt for the language model\n",
        "        prompt = f\"\"\"\n",
        "        Context: {' '.join(context)}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Please provide a concise answer to the question based on the given context.\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        # Use the language model to generate an answer based on the prompt\n",
        "        return self.lm(prompt)\n",
        "\n",
        "    def __call__(self, question):\n",
        "        \"\"\"\n",
        "        Make the class callable. This method orchestrates the RAG process.\n",
        "\n",
        "        :param question: The input question\n",
        "        :return: A dictionary containing the question, retrieved context, and generated answer\n",
        "        \"\"\"\n",
        "        # First, retrieve relevant contexts\n",
        "        context = self.retrieve(question)\n",
        "\n",
        "        # Then, generate an answer based on the question and retrieved contexts\n",
        "        answer = self.generate_answer(question, context)\n",
        "\n",
        "        # Return all information in a dictionary\n",
        "        return {'question': question, 'context': context, 'answer': answer}\n",
        "\n",
        "# Usage example:\n",
        "# Note: You need to have these components set up before using this class:\n",
        "# - index: Your FAISS index\n",
        "# - sentences: Your list of sentences or documents\n",
        "# - model: Your sentence transformer model\n",
        "# - dspy.settings.lm: Your language model (GPT-2, Llama, etc.) configured in DSPy\n",
        "\n",
        "# Create an instance of the AdvancedRAG class\n",
        "advanced_rag = AdvancedRAG(index, sentences, model, dspy.settings.lm)\n",
        "\n",
        "# Test the advanced RAG system with a sample question\n",
        "question = \"What are the main areas of AI?\"\n",
        "result = advanced_rag(question)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Question: {result['question']}\")\n",
        "print(f\"Context: {result['context']}\")\n",
        "print(f\"Answer: {result['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqQD28pwcgyc"
      },
      "source": [
        "### Exercise 4.1: Implement Multi-Stage Retrieval\n",
        "\n",
        "Extend the AdvancedRAG class to implement a multi-stage retrieval process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TF7hYjJbCvT"
      },
      "outputs": [],
      "source": [
        "import dspy\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Custom HuggingFace language model for DSPy\n",
        "class HFLanguageModel:\n",
        "    def __init__(self, model_name=\"google/flan-t5-base\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = pipeline(\"text2text-generation\", model=model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def generate(self, prompt, **kwargs):\n",
        "        response = self.model(prompt, max_length=100, **kwargs)[0]['generated_text']\n",
        "        return response\n",
        "\n",
        "# Custom retriever using HuggingFace embeddings and FAISS\n",
        "class HFRetriever:\n",
        "    def __init__(self, sentences, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.sentences = sentences\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.index = self._create_index()\n",
        "\n",
        "    def _create_index(self):\n",
        "        embeddings = self.model.encode(self.sentences)\n",
        "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "        index.add(embeddings)\n",
        "        return index\n",
        "\n",
        "    def retrieve(self, query, k=3):\n",
        "        query_embedding = self.model.encode([query])\n",
        "        _, indices = self.index.search(query_embedding, k)\n",
        "        return [self.sentences[idx] for idx in indices[0]]\n",
        "\n",
        "# Set up our language model and retriever\n",
        "lm = HFLanguageModel()\n",
        "sentences = [\"AI is a broad field of computer science.\", \"Machine learning is a subset of AI.\", \"Deep learning uses neural networks with multiple layers.\"]\n",
        "retriever = HFRetriever(sentences)\n",
        "\n",
        "class RAG(dspy.Module):\n",
        "    def __init__(self, retriever, lm):\n",
        "        super().__init__()\n",
        "        self.retriever = retriever\n",
        "        self.lm = lm\n",
        "\n",
        "    def forward(self, question):\n",
        "        context = self.retriever.retrieve(question)\n",
        "        prompt = f\"Context: {' '.join(context)}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "        answer = self.lm.generate(prompt)\n",
        "        return answer\n",
        "\n",
        "# Create our RAG instance\n",
        "rag = RAG(retriever, lm)\n",
        "\n",
        "# Use the RAG system\n",
        "question = \"What is the relationship between machine learning and AI?\"\n",
        "answer = rag(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "\n",
        "class MultiStageRAG(dspy.Module):\n",
        "    def __init__(self, retriever, lm):\n",
        "        super().__init__()\n",
        "        self.retriever = retriever\n",
        "        self.lm = lm\n",
        "\n",
        "    def rerank(self, passages, question):\n",
        "        prompt = f\"Question: {question}\\n\\nPassages: {' '.join(passages)}\\n\\nRank these passages by relevance to the question. Return only the two most relevant passages, separated by a newline:\"\n",
        "        reranked = self.lm.generate(prompt).strip().split('\\n')\n",
        "        return reranked[:2]\n",
        "\n",
        "    def forward(self, question):\n",
        "        initial_context = self.retriever.retrieve(question, k=5)\n",
        "        reranked_context = self.rerank(initial_context, question)\n",
        "        prompt = f\"Context: {' '.join(reranked_context)}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "        answer = self.lm.generate(prompt)\n",
        "        return answer\n",
        "\n",
        "# Create our multi-stage RAG instance\n",
        "multi_stage_rag = MultiStageRAG(retriever, lm)\n",
        "\n",
        "# Use the multi-stage RAG system\n",
        "question = \"What are the main areas of AI?\"\n",
        "answer = multi_stage_rag(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}