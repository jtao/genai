{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtao/genai/blob/main/1.%20LLM-Components.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSOnt6mXiyPS"
      },
      "source": [
        "# Module 1: LLM Components\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "Welcome to this comprehensive LLM Orchestration Teaching Kit. This notebook is designed to provide an in-depth understanding of Large Language Model (LLM) orchestration techniques. By the end of this session, you'll have hands-on experience with:\n",
        "\n",
        "1. Advanced prompt engineering and templating\n",
        "2. The inner workings of tokenizers and transformers\n",
        "3. Few-shot and zero-shot learning techniques\n",
        "4. Complex chain implementations using LangChain\n",
        "5. Evaluation and fine-tuning of LLM outputs\n",
        "\n",
        "This notebook is designed to take over an hour to complete, providing a deep dive into each topic with practical exercises and advanced concepts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWBqyJQgw2z-"
      },
      "source": [
        "\n",
        "\n",
        "# Setup\n",
        "\n",
        "First, let's install the necessary **libraries**, **import** them, and **load in our LLM**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qboTJhJzxOrf"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq transformers langchain openai evaluate datasets tiktoken langchain-huggingface bitsandbytes accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "r0M8TO637NaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVNBCStrxSOz"
      },
      "outputs": [],
      "source": [
        "# Now, let's import the required libraries:\n",
        "\n",
        "import torch\n",
        "import bitsandbytes\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Tokenizer,TextStreamer ,pipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
        "from langchain.prompts import FewShotPromptTemplate\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "import os\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WuKQueIVZIAq"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "#model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\" , load_in_4bit=True)\n",
        "streamer = TextStreamer(tokenizer)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    max_length=1024,\n",
        "    truncation=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        "    streamer=streamer,\n",
        ")\n",
        "\n",
        "# Create a LangChain wrapper for the Hugging Face pipeline\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r95oRwoxTnU"
      },
      "source": [
        "# 1. Advanced Prompt Engineering and Templating\n",
        "\n",
        "Prompt engineering is a crucial skill in working with LLMs. Let's explore various techniques to create effective prompts.\n",
        "\n",
        "## 1.1 Simple vs. Structured Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-MPGxAtxVt_"
      },
      "outputs": [],
      "source": [
        "# Simple prompt\n",
        "simple_prompt = \"Write a short story about a robot.\"\n",
        "\n",
        "# Structured prompt\n",
        "structured_prompt = \"\"\"\n",
        "Task: Write a short story\n",
        "Topic: A robot\n",
        "Genre: Science fiction\n",
        "Length: Approximately 100 words\n",
        "Key elements to include:\n",
        "- The robot's purpose\n",
        "- A moral dilemma the robot faces\n",
        "- Resolution of the dilemma\n",
        "\"\"\"\n",
        "\n",
        "# Compare outputs\n",
        "print(\"Simple Prompt Output:\")\n",
        "print(llm(simple_prompt))\n",
        "print(\"\\nStructured Prompt Output:\")\n",
        "print(llm(structured_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY6n8M5axp0W"
      },
      "source": [
        "## 1.2 Advanced Templating with Dynamic Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcO4vO-AxssC"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "advanced_template = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"genre\", \"length\", \"key_elements\"],\n",
        "    template=\"\"\"\n",
        "Task: Write a short story\n",
        "Topic: {topic}\n",
        "Genre: {genre}\n",
        "Length: Approximately {length} words\n",
        "Key elements to include:\n",
        "{key_elements}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Example usage\n",
        "story_prompt = advanced_template.format(\n",
        "    topic=\"A time-traveling historian\",\n",
        "    genre=\"Historical fiction\",\n",
        "    length=\"150\",\n",
        "    key_elements=\"- A significant historical event\\n- An ethical decision about changing the past\\n- Consequences of the historian's actions\"\n",
        ")\n",
        "\n",
        "print(story_prompt)\n",
        "print(llm(story_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRed-ZahxwV0"
      },
      "source": [
        "## 1.3 Few-Shot Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91NSfvxIx0tA"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# Define our list of few-shot examples\n",
        "examples = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
        "    {\"word\": \"rich\", \"antonym\": \"poor\"},\n",
        "]\n",
        "\n",
        "# Create our example template\n",
        "example_template = \"\"\"\n",
        "Word: {word}\n",
        "Antonym: {antonym}\n",
        "\"\"\"\n",
        "\n",
        "# Create a prompt example from the above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_template,\n",
        ")\n",
        "\n",
        "# Finally, create the few-shot prompt template\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym for each word:\",\n",
        "    suffix=\"Word: {input}\\nAntonym:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n",
        "\n",
        "# Test it out!\n",
        "print(few_shot_prompt.format(input=\"big\"))\n",
        "print(llm(few_shot_prompt.format(input=\"big\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgIocKiOze0j"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ed5sCEFx3fw"
      },
      "source": [
        "\n",
        "\n",
        "# 2. Tokenizers and Transformers: A Deep Dive\n",
        "\n",
        "## 2.1 Understanding Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsaLvYkrx6iM"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "text = \"Let's tokenize this sentence and understand how it works!\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "token_ids = tokenizer.encode(text)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "\n",
        "# Decode back to text\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(\"Decoded text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIATiY9vyIim"
      },
      "source": [
        "## 2.2 Generating Text with Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QFkqrVHyMGC"
      },
      "outputs": [],
      "source": [
        "input_text = \"In a world where AI\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate multiple sequences\n",
        "output_sequences = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=2,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "for i, seq in enumerate(output_sequences):\n",
        "    print(f\"Generated sequence {i + 1}:\")\n",
        "    print(tokenizer.decode(seq, skip_special_tokens=True))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCsFBty4zOSp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFBrqIiyyPLk"
      },
      "source": [
        "\n",
        "\n",
        "# 3. Advanced Few-Shot and Zero-Shot Learning\n",
        "\n",
        "## 3.1 Zero-Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvPm0jVaySqb"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "sequence = \"I loved the new Batman movie!\"\n",
        "candidate_labels = [\"positive\", \"negative\", \"neutral\"]\n",
        "\n",
        "result = classifier(sequence, candidate_labels)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLfBVMOyVvx"
      },
      "source": [
        "## 3.2 Few-Shot Learning with Dynamic Example Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4jfA7afyYhh"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "# Prepare a larger set of examples\n",
        "examples = [\n",
        "    {\"input\": \"I love this movie!\", \"output\": \"Positive\"},\n",
        "    {\"input\": \"This film is terrible.\", \"output\": \"Negative\"},\n",
        "    {\"input\": \"The acting was great but the plot was confusing.\", \"output\": \"Mixed\"},\n",
        "    {\"input\": \"I've seen better.\", \"output\": \"Negative\"},\n",
        "    {\"input\": \"A masterpiece of cinema!\", \"output\": \"Positive\"},\n",
        "    {\"input\": \"It was okay, nothing special.\", \"output\": \"Neutral\"},\n",
        "]\n",
        "\n",
        "# Create our example template\n",
        "example_template = \"\"\"\n",
        "Input: {input}\n",
        "Output: {output}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=example_template,\n",
        ")\n",
        "\n",
        "# Create a LengthBasedExampleSelector\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=50  # This sets the max length of examples to use\n",
        ")\n",
        "\n",
        "# Create the few-shot prompt template\n",
        "dynamic_few_shot_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Classify the sentiment in these movie reviews:\",\n",
        "    suffix=\"Input: {input}\\nOutput:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n",
        "\n",
        "# Test it out with different inputs\n",
        "print(dynamic_few_shot_prompt.format(input=\"This movie changed my life!\"))\n",
        "print(llm(dynamic_few_shot_prompt.format(input=\"This movie changed my life!\")))\n",
        "\n",
        "print(dynamic_few_shot_prompt.format(input=\"I fell asleep halfway through.\"))\n",
        "print(llm(dynamic_few_shot_prompt.format(input=\"I fell asleep halfway through.\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHYFIiwFzhMP"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8lQngqOypf5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# 4. Advanced Chain Implementations with LangChain\n",
        "\n",
        "## 4.1 Multi-Stage Processing Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky2plEu7yrIq"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain, LLMChain\n",
        "\n",
        "# First chain: Generate a news headline\n",
        "headline_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Generate a catchy news headline about {topic}.\"\n",
        ")\n",
        "headline_chain = LLMChain(llm=llm, prompt=headline_prompt)\n",
        "\n",
        "# Second chain: Generate a short news article\n",
        "article_prompt = PromptTemplate(\n",
        "    input_variables=[\"headline\"],\n",
        "    template=\"Write a short news article based on this headline: {headline}\"\n",
        ")\n",
        "article_chain = LLMChain(llm=llm, prompt=article_prompt)\n",
        "\n",
        "# Third chain: Generate three hashtags for social media\n",
        "hashtag_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Generate three relevant hashtags for this news article: {article}\"\n",
        ")\n",
        "hashtag_chain = LLMChain(llm=llm, prompt=hashtag_prompt)\n",
        "\n",
        "# Combine the chains\n",
        "news_chain = SimpleSequentialChain(chains=[headline_chain, article_chain, hashtag_chain], verbose=True)\n",
        "\n",
        "# Run the chain\n",
        "print(news_chain.run(\"artificial intelligence\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLUFjSMsyvcr"
      },
      "source": [
        "## 4.2 Branching Chain with Conditional Logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjk3LZUVy0Th"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain, SequentialChain\n",
        "\n",
        "# Chain 1: Determine the type of task\n",
        "task_prompt = PromptTemplate(\n",
        "    input_variables=[\"objective\"],\n",
        "    template=\"Determine if the following objective requires 'analysis' or 'creation': {objective}\"\n",
        ")\n",
        "task_chain = LLMChain(llm=llm, prompt=task_prompt)\n",
        "\n",
        "# Chain 2A: Analysis task\n",
        "analysis_prompt = PromptTemplate(\n",
        "    input_variables=[\"objective\"],\n",
        "    template=\"Provide a detailed analysis of the following: {objective}\"\n",
        ")\n",
        "analysis_chain = LLMChain(llm=llm, prompt=analysis_prompt)\n",
        "\n",
        "# Chain 2B: Creation task\n",
        "creation_prompt = PromptTemplate(\n",
        "    input_variables=[\"objective\"],\n",
        "    template=\"Create a detailed plan or outline for the following: {objective}\"\n",
        ")\n",
        "creation_chain = LLMChain(llm=llm, prompt=creation_prompt)\n",
        "\n",
        "# Chain 3: Summarize the output\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"result\"],\n",
        "    template=\"Summarize the key points from this output: {result}\"\n",
        ")\n",
        "summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
        "\n",
        "# Combining chains with conditional logic\n",
        "def branching_chain(objective):\n",
        "    task_type = task_chain.run(objective).strip().lower()\n",
        "    if 'analysis' in task_type:\n",
        "        result = analysis_chain.run(objective)\n",
        "    else:\n",
        "        result = creation_chain.run(objective)\n",
        "    summary = summary_chain.run(result)\n",
        "    return f\"Task Type: {task_type}\\n\\nResult: {result}\\n\\nSummary: {summary}\"\n",
        "\n",
        "# Test the branching chain\n",
        "print(branching_chain(\"Evaluate the impact of social media on modern politics\"))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(branching_chain(\"Design a marketing campaign for a new electric car\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlFnf84zjYu"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}